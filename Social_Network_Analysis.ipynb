{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Compute Network Statistics such as Centrality (Degree, Closeness & Betweenness) and Proximity Prestige for a suitable Social Network.**"
      ],
      "metadata": {
        "id": "45WN7UK7MhFV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1-mWm_6uowr",
        "outputId": "2bbcfa00-481e-432a-a998-9d072a155116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Degree Centrality: {'A': 0.6000000000000001, 'B': 0.6000000000000001, 'C': 0.6000000000000001, 'D': 0.8, 'E': 0.6000000000000001, 'F': 0.4}\n",
            "Closeness Centrality: {'A': 0.7142857142857143, 'B': 0.7142857142857143, 'C': 0.7142857142857143, 'D': 0.8333333333333334, 'E': 0.7142857142857143, 'F': 0.625}\n",
            "Betweenness Centrality: {'A': 0.03333333333333333, 'B': 0.1, 'C': 0.03333333333333333, 'D': 0.30000000000000004, 'E': 0.13333333333333333, 'F': 0.0}\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Example social network data (replace with your data)\n",
        "nodes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
        "edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"A\", \"D\"),\n",
        "         (\"B\", \"C\"), (\"B\", \"E\"), (\"C\", \"D\"),\n",
        "         (\"D\", \"E\"), (\"D\", \"F\"), (\"E\", \"F\")]\n",
        "\n",
        "# Create a network graph\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Centrality Measures\n",
        "\n",
        "# Degree Centrality: Counts the number of connections for each node\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "print(\"Degree Centrality:\", degree_centrality)\n",
        "\n",
        "# Closeness Centrality: Measures how close a node is to all other nodes (reciprocal of average shortest path)\n",
        "closeness_centrality = nx.closeness_centrality(G)\n",
        "print(\"Closeness Centrality:\", closeness_centrality)\n",
        "\n",
        "# Betweenness Centrality: Measures how often a node lies on the shortest path between other nodes\n",
        "betweenness_centrality = nx.betweenness_centrality(G)\n",
        "print(\"Betweenness Centrality:\", betweenness_centrality)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Implement Random Walk for a suitable Social Network. Compute Hitting and Commute time for some starting and target node.**"
      ],
      "metadata": {
        "id": "eMbj3JnAMqT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "\n",
        "# Example social network data (replace with your data)\n",
        "nodes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
        "edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"A\", \"D\"),\n",
        "         (\"B\", \"C\"), (\"B\", \"E\"), (\"C\", \"D\"),\n",
        "         (\"D\", \"E\"), (\"D\", \"F\"), (\"E\", \"F\")]\n",
        "\n",
        "# Create a network graph (assuming undirected)\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Define a random walk function\n",
        "def random_walk(G, start_node, max_steps=100):\n",
        "  current_node = start_node\n",
        "  visited_nodes = [current_node]\n",
        "  for _ in range(max_steps):\n",
        "    neighbors = list(G.neighbors(current_node))\n",
        "    if not neighbors:\n",
        "      break\n",
        "    next_node = random.choice(neighbors)\n",
        "    current_node = next_node\n",
        "    visited_nodes.append(current_node)\n",
        "  return visited_nodes\n",
        "\n",
        "# Define a function to calculate hitting time\n",
        "def hitting_time(G, start_node, target_node, max_steps=100):\n",
        "  walk = random_walk(G, start_node, max_steps)\n",
        "  for i, node in enumerate(walk):\n",
        "    if node == target_node:\n",
        "      return i\n",
        "  return None\n",
        "\n",
        "# Undirected Graphs: Alternative approaches for Commute Time\n",
        "# Option 1: Repeated Random Walks with Reversed Walk Check (may be inefficient)\n",
        "def commute_time_estimate_rw(G, start_node, target_node, max_steps=100, num_walks=10):\n",
        "  total_time = 0\n",
        "  for _ in range(num_walks):\n",
        "    first_hit_time = hitting_time(G, start_node, target_node, max_steps)\n",
        "    if first_hit_time is None:\n",
        "      continue  # Skip if target not reached\n",
        "    reversed_walk = list(reversed(random_walk(G, target_node, max_steps)))\n",
        "    second_hit_time = None\n",
        "    for i, node in enumerate(reversed_walk):\n",
        "      if node == start_node:\n",
        "        second_hit_time = i\n",
        "        break\n",
        "    if second_hit_time is not None:\n",
        "      total_time += first_hit_time + second_hit_time\n",
        "  if total_time > 0:\n",
        "    return total_time / num_walks\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "# Option 2: Create a temporary directed copy for commute time calculation (modifies original graph)\n",
        "def commute_time_directed_copy(G, start_node, target_node, max_steps=100):\n",
        "  \"\"\"\n",
        "  Calculates commute time using a temporary directed copy of the graph.\n",
        "  This approach modifies the original graph (creates a directed copy).\n",
        "  \"\"\"\n",
        "  directed_G = G.to_directed()  # Create a directed copy for commute time calculation\n",
        "  commute_time_val = nx.shortest_path_length(directed_G, start_node, target_node, weight=None)\n",
        "  # Consider handling cases where no path exists (commute_time_val will raise an exception)\n",
        "  return commute_time_val\n",
        "\n",
        "# Example usage\n",
        "start_node = \"A\"\n",
        "target_node = \"E\"\n",
        "\n",
        "walk = random_walk(G, start_node)\n",
        "print(\"Random Walk:\", walk)\n",
        "\n",
        "hit_time = hitting_time(G, start_node, target_node)\n",
        "print(\"Hitting Time from\", start_node, \"to\", target_node, \":\", hit_time)\n",
        "\n",
        "# Option 1: Commute Time Estimate (may be inefficient for large graphs)\n",
        "commute_time_est = commute_time_estimate_rw(G, start_node, target_node)\n",
        "print(\"Estimated Commute Time (Random Walks) between\", start_node, \"and\", target_node, \":\", commute_time_est)\n",
        "\n",
        "# Option 2: Commute Time using Directed Copy (modifies original graph)\n",
        "commute_time_val = commute_time_directed_copy(G, start_node, target_node)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVDSNwAm4YDI",
        "outputId": "ca6d65f8-d0da-4cf6-dc7e-455b8cb2f6b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Walk: ['A', 'D', 'F', 'D', 'E', 'D', 'E', 'F', 'D', 'E', 'D', 'F', 'D', 'A', 'D', 'C', 'B', 'E', 'F', 'E', 'F', 'E', 'F', 'E', 'B', 'C', 'D', 'F', 'D', 'E', 'D', 'A', 'C', 'D', 'A', 'D', 'F', 'E', 'F', 'E', 'F', 'D', 'A', 'D', 'A', 'B', 'C', 'A', 'C', 'B', 'A', 'D', 'E', 'B', 'C', 'D', 'E', 'B', 'E', 'B', 'E', 'F', 'E', 'B', 'E', 'B', 'A', 'D', 'F', 'D', 'F', 'D', 'F', 'E', 'D', 'F', 'E', 'B', 'A', 'C', 'B', 'A', 'B', 'E', 'D', 'E', 'D', 'C', 'D', 'A', 'C', 'D', 'C', 'A', 'D', 'C', 'D', 'E', 'D', 'F', 'D']\n",
            "Hitting Time from A to E : 4\n",
            "Estimated Commute Time (Random Walks) between A and E : 10.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Above code for Directed Graphs:\n",
        "\n",
        "import networkx as nx\n",
        "import random\n",
        "\n",
        "# Example social network data (replace with your data)\n",
        "nodes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
        "edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"A\", \"D\"),\n",
        "         (\"B\", \"C\"), (\"B\", \"E\"), (\"C\", \"D\"),\n",
        "         (\"D\", \"E\"), (\"D\", \"F\"), (\"E\", \"F\")]\n",
        "\n",
        "# Create a directed network graph\n",
        "G = nx.DiGraph()\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Define a random walk function (modified for directed graphs)\n",
        "def random_walk(G, start_node, max_steps=100):\n",
        "  current_node = start_node\n",
        "  visited_nodes = [current_node]\n",
        "  for _ in range(max_steps):\n",
        "    neighbors = list(G.successors(current_node))  # Successors for directed graphs\n",
        "    if not neighbors:\n",
        "      break\n",
        "    next_node = random.choice(neighbors)\n",
        "    current_node = next_node\n",
        "    visited_nodes.append(current_node)\n",
        "  return visited_nodes\n",
        "\n",
        "# Define a function to calculate hitting time (directed graphs)\n",
        "def hitting_time(G, start_node, target_node, max_steps=100):\n",
        "  walk = random_walk(G, start_node, max_steps)\n",
        "  for i, node in enumerate(walk):\n",
        "    if node == target_node:\n",
        "      return i\n",
        "  return None\n",
        "\n",
        "# Commute time calculation is not directly applicable for directed graphs\n",
        "# Consider alternative measures like:\n",
        "# - Directed betweenness centrality (for influence on shortest paths)\n",
        "# - Random walk with restart (biased walk favoring starting node)\n",
        "\n",
        "# Example usage\n",
        "start_node = \"A\"\n",
        "target_node = \"E\"\n",
        "\n",
        "walk = random_walk(G, start_node)\n",
        "print(\"Random Walk:\", walk)\n",
        "\n",
        "hit_time = hitting_time(G, start_node, target_node)\n",
        "print(\"Hitting Time from\", start_node, \"to\", target_node, \":\", hit_time)\n",
        "\n",
        "# Commenting out commute time calculation as it's not directly applicable for directed graphs\n",
        "# commute_time_val = commute_time(G, start_node, target_node)\n",
        "# print(\"Commute Time between\", start_node, \"and\", target_node, \":\", commute_time_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yJlvp4w4pyy",
        "outputId": "4662df50-9f05-4556-b9f7-654cd8448f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Walk: ['A', 'B', 'E', 'F']\n",
            "Hitting Time from A to E : None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Compute Importance and Relatedness Neumann Kernal proximity measures for suitable Social Network.**"
      ],
      "metadata": {
        "id": "0NttrT-HM3Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Example social network data (replace with your data)\n",
        "nodes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
        "edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"A\", \"D\"),\n",
        "         (\"B\", \"C\"), (\"B\", \"E\"), (\"C\", \"D\"),\n",
        "         (\"D\", \"E\"), (\"D\", \"F\"), (\"E\", \"F\")]\n",
        "\n",
        "# Create a network graph\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Define a function to calculate Neumann kernel proximity matrix\n",
        "def neumann_kernel(G, alpha):\n",
        "  \"\"\"\n",
        "  Calculates Neumann kernel proximity matrix for a graph.\n",
        "\n",
        "  Args:\n",
        "      G: NetworkX graph object representing the social network.\n",
        "      alpha: Damping factor (between 0 and 1).\n",
        "\n",
        "  Returns:\n",
        "      A NumPy array representing the Neumann kernel proximity matrix.\n",
        "  \"\"\"\n",
        "  # Create adjacency matrix (consider using sparse matrix for large graphs)\n",
        "  A = nx.adjacency_matrix(G).todense()\n",
        "  # Adjust for self-loops (set diagonal to 0)\n",
        "  np.fill_diagonal(A, 0)\n",
        "  # Calculate Neumann kernel (I - alpha * A)^(-1) using matrix inversion\n",
        "  I = np.identity(A.shape[0])\n",
        "  return np.linalg.inv(I - alpha * A)\n",
        "\n",
        "# Example usage\n",
        "alpha = 0.8  # Damping factor (usually between 0 and 1)\n",
        "\n",
        "# Calculate Neumann kernel proximity matrix\n",
        "neumann_matrix = neumann_kernel(G, alpha)\n",
        "\n",
        "# Access individual proximity values between nodes\n",
        "node_a = \"A\"  # Example node\n",
        "\n",
        "# Convert NodeView to list for efficient indexing (NetworkX versions 2.x and above)\n",
        "node_list = list(G.nodes())\n",
        "node_a_index = node_list.index(node_a)\n",
        "\n",
        "# Print results (example for node A)\n",
        "print(\"Neumann Kernel Proximity (alpha =\", alpha, \"):\")\n",
        "for i, node in enumerate(node_list):\n",
        "  if i != node_a_index:  # Avoid self-proximity\n",
        "    print(\"  \", node, \":\", neumann_matrix[node_a_index][i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYZA3LVz4t_q",
        "outputId": "a11c76ce-9aad-48a3-b062-f8582e21769e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neumann Kernel Proximity (alpha = 0.8 ):\n",
            "   B : 1.1792733770101271\n",
            "   C : 1.7457481305009621\n",
            "   D : -1.2983918999404422\n",
            "   E : -2.572960095294821\n",
            "   F : -3.097081596188211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Implement Page Rank Algorithm for rating the importance of web pages using directed graph.**"
      ],
      "metadata": {
        "id": "rAxBxJ4vM_td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Example web page data (replace with your data)\n",
        "webpages = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
        "links = [(\"A\", \"B\"), (\"A\", \"C\"), (\"A\", \"D\"),\n",
        "         (\"B\", \"C\"), (\"B\", \"E\"), (\"C\", \"D\")]\n",
        "\n",
        "# Create a directed graph representing web page links\n",
        "G = nx.DiGraph()\n",
        "G.add_nodes_from(webpages)\n",
        "G.add_edges_from(links)\n",
        "\n",
        "# Define a function to calculate PageRank\n",
        "def pagerank(G, alpha=0.8, max_iterations=100, tol=1e-4):\n",
        "  \"\"\"\n",
        "  Calculates PageRank for web pages in a directed graph.\n",
        "\n",
        "  Args:\n",
        "      G: NetworkX DiGraph object representing web pages and links.\n",
        "      alpha: Damping factor (between 0 and 1).\n",
        "      max_iterations: Maximum number of iterations (default: 100).\n",
        "      tol: Tolerance for convergence (default: 1e-4).\n",
        "\n",
        "  Returns:\n",
        "      A dictionary containing PageRank scores for each web page.\n",
        "  \"\"\"\n",
        "  # Initialize PageRank scores with equal probability\n",
        "  pr = dict.fromkeys(G.nodes(), 1 / len(G.nodes()))\n",
        "  for _ in range(max_iterations):\n",
        "    new_pr = {node: 0 for node in G.nodes()}\n",
        "    for node in G.nodes():\n",
        "      # Consider in-links (pages linking to the current node)\n",
        "      in_links = G.in_edges(node)\n",
        "      if not in_links:\n",
        "        continue  # Handle dangling nodes (no in-links)\n",
        "      # Avoid division by zero for nodes with no outgoing links\n",
        "      if not G.out_edges(node):\n",
        "        # Distribute PageRank of dangling node equally among its in-links\n",
        "        for _, source in in_links:\n",
        "          new_pr[source] += pr[node] / len(G.in_edges(source))\n",
        "        continue\n",
        "      for _, source in in_links:\n",
        "        new_pr[node] += alpha * pr[source] / len(G.out_edges(source))\n",
        "    # Add damping factor and normalize\n",
        "    for node in new_pr:\n",
        "      new_pr[node] = (1 - alpha) + alpha * new_pr[node]\n",
        "    # Check for convergence (change in PageRank is less than tolerance)\n",
        "    if sum(abs(new_pr[v] - pr[v]) for v in pr) < tol:\n",
        "      break\n",
        "    pr = new_pr\n",
        "  return pr\n",
        "\n",
        "# Example usage\n",
        "alpha = 0.8  # Damping factor\n",
        "\n",
        "# Calculate PageRank scores\n",
        "pagerank_scores = pagerank(G, alpha)\n",
        "\n",
        "# Print results\n",
        "print(\"PageRank Scores:\")\n",
        "for page, score in pagerank_scores.items():\n",
        "  print(\"  \", page, \":\", score)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7dqmgkS4zMZ",
        "outputId": "735caef1-d977-41d2-c97b-62403d206a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PageRank Scores:\n",
            "   A : 0.19999999999999996\n",
            "   B : 0.2941176470588235\n",
            "   C : 48092671099.15701\n",
            "   D : 0.9999999998370371\n",
            "   E : 0.9999999998370371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Implement Markov Cluster Algorithm over undirected graph.**"
      ],
      "metadata": {
        "id": "fwdiZ4q1NPRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def markov_clustering(adjacency_matrix, inflation=1.8, max_iterations=100, tol=1e-4):\n",
        "  \"\"\"\n",
        "  Performs Markov Clustering on an undirected graph represented by an adjacency matrix.\n",
        "\n",
        "  Args:\n",
        "      adjacency_matrix: A NumPy array representing the adjacency matrix of the graph.\n",
        "      inflation: Inflation parameter (usually between 1.2 and 2.0).\n",
        "      max_iterations: Maximum number of iterations (default: 100).\n",
        "      tol: Tolerance for convergence (default: 1e-4).\n",
        "\n",
        "  Returns:\n",
        "      A NumPy array containing cluster assignments for each node.\n",
        "  \"\"\"\n",
        "  # Normalize the adjacency matrix (transition matrix)\n",
        "  normalized_matrix = normalize_adjacency_matrix(adjacency_matrix)\n",
        "\n",
        "  # Perform inflation\n",
        "  for _ in range(max_iterations):\n",
        "    inflated_matrix = normalized_matrix**inflation\n",
        "    # Check for convergence (change in inflated matrix is less than tolerance)\n",
        "    if np.linalg.norm(inflated_matrix - normalized_matrix) < tol:\n",
        "      break\n",
        "    normalized_matrix = inflated_matrix\n",
        "\n",
        "  # Update with final inflated matrix\n",
        "  inflated_matrix = normalized_matrix\n",
        "\n",
        "  # Perform deflation (optional for further refinement, uncomment if desired)\n",
        "  # for _ in range(max_iterations):\n",
        "  #   deflated_matrix = inflated_matrix.copy()\n",
        "  #   for i in range(len(inflated_matrix)):\n",
        "  #     deflated_matrix[i, :] = inflated_matrix[i, :] / np.sum(inflated_matrix[i, :])\n",
        "  #   inflated_matrix = deflated_matrix\n",
        "  #   # Check for convergence (change in deflated matrix is less than tolerance)\n",
        "  #   if np.linalg.norm(deflated_matrix - inflated_matrix) < tol:\n",
        "  #     break\n",
        "\n",
        "  # Assign clusters based on the largest element in each row\n",
        "  clusters = np.argmax(inflated_matrix, axis=1)\n",
        "  return clusters\n",
        "\n",
        "def normalize_adjacency_matrix(adjacency_matrix):\n",
        "  \"\"\"\n",
        "  Normalizes an adjacency matrix of an undirected graph.\n",
        "\n",
        "  Args:\n",
        "      adjacency_matrix: A NumPy array representing the adjacency matrix.\n",
        "\n",
        "  Returns:\n",
        "      A NumPy array representing the normalized adjacency matrix.\n",
        "  \"\"\"\n",
        "  degree_matrix = np.diag(np.sum(adjacency_matrix, axis=0))\n",
        "  # Avoid division by zero for isolated nodes\n",
        "  degree_matrix[np.diag(degree_matrix) == 0] = 1\n",
        "  normalized_matrix = adjacency_matrix @ np.linalg.inv(degree_matrix)\n",
        "  return normalized_matrix\n",
        "\n",
        "# Example usage (replace with your data)\n",
        "nodes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
        "edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"A\", \"D\"),\n",
        "         (\"B\", \"C\"), (\"B\", \"E\"), (\"C\", \"D\"),\n",
        "         (\"D\", \"E\"), (\"D\", \"F\"), (\"E\", \"F\")]\n",
        "\n",
        "# Create adjacency matrix\n",
        "adjacency_matrix = np.zeros((len(nodes), len(nodes)))\n",
        "for edge in edges:\n",
        "  i, j = nodes.index(edge[0]), nodes.index(edge[1])\n",
        "  adjacency_matrix[i, j] = 1\n",
        "  adjacency_matrix[j, i] = 1  # Undirected graph (symmetric)\n",
        "\n",
        "# Define parameters\n",
        "inflation = 1.8\n",
        "max_iterations = 100\n",
        "tol = 1e-4\n",
        "\n",
        "# Run MCL\n",
        "clusters = markov_clustering(adjacency_matrix, inflation, max_iterations, tol)\n",
        "\n",
        "# Print cluster assignments\n",
        "print(\"Cluster Assignments:\")\n",
        "for i, node in enumerate(nodes):\n",
        "  print(\"  \", node, \":\", clusters[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uXbbkDK42u-",
        "outputId": "bfbdb467-a537-4dd2-b6d8-0dc140dc4efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Assignments:\n",
            "   A : 1\n",
            "   B : 0\n",
            "   C : 0\n",
            "   D : 5\n",
            "   E : 5\n",
            "   F : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Compare Proximity Measures such as graph distance, common neighbors Jaccard's coefficient for Link Prediction using suitable dataset.**"
      ],
      "metadata": {
        "id": "b9ZOrWVeNYTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "nodes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
        "edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"A\", \"D\"),\n",
        "         (\"B\", \"C\"), (\"B\", \"E\"), (\"C\", \"D\"),\n",
        "         (\"D\", \"E\"), (\"D\", \"F\"), (\"E\", \"F\")]\n",
        "\n",
        "# Create a network graph\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Define functions for calculating proximity measures\n",
        "def graph_distance(G, node1, node2):\n",
        "  try:\n",
        "    return nx.shortest_path_length(G, node1, node2)\n",
        "  except nx.NetworkXNoPath:\n",
        "    return float('inf')  # Handle unconnected nodes\n",
        "\n",
        "def common_neighbors(G, node1, node2):\n",
        "  return len(set(G.neighbors(node1)) & set(G.neighbors(node2)))\n",
        "\n",
        "def jaccard_coefficient(G, node1, node2):\n",
        "  neighbors1 = set(G.neighbors(node1))\n",
        "  neighbors2 = set(G.neighbors(node2))\n",
        "  union = neighbors1 | neighbors2\n",
        "  intersection = neighbors1 & neighbors2\n",
        "  if len(union) == 0:\n",
        "    return 0  # Avoid division by zero\n",
        "  return len(intersection) / len(union)\n",
        "\n",
        "# Define function to evaluate link prediction performance\n",
        "def evaluate_link_prediction(G, test_edges, measure_func):\n",
        "  correct_predictions = 0\n",
        "  for edge in test_edges:\n",
        "    node1, node2 = edge\n",
        "    if measure_func(G, node1, node2) > 0:  # Consider non-zero proximity for prediction\n",
        "      correct_predictions += 1\n",
        "  accuracy = correct_predictions / len(test_edges)\n",
        "  return accuracy\n",
        "\n",
        "# Split data into training and testing sets (replace with your splitting criteria)\n",
        "train_edges, test_edges = train_test_split(list(G.edges()), test_size=0.2)\n",
        "\n",
        "# Compare proximity measures on the test set\n",
        "measures = {\"Graph Distance\": graph_distance, \"Common Neighbors\": common_neighbors, \"Jaccard Coefficient\": jaccard_coefficient}\n",
        "for name, measure_func in measures.items():\n",
        "  accuracy = evaluate_link_prediction(G, test_edges, measure_func)\n",
        "  print(f\"{name} Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUYeuAa-46jk",
        "outputId": "d5bb3467-9884-4a97-dc1a-94509ef6c642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph Distance Accuracy: 1.0\n",
            "Common Neighbors Accuracy: 0.5\n",
            "Jaccard Coefficient Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Implement Univariate Temporal Methods for Event Detection over suitable dataset.**"
      ],
      "metadata": {
        "id": "tl6SWEd5NgbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample time series data (replace with your actual data)\n",
        "data = [10, 12, 15, 18, 20, 17, 15, 12, 8, 5, 3, 2, 1, 4, 8, 12, 15]\n",
        "timestamps = range(len(data))  # Assuming timestamps correspond to data indices\n",
        "\n",
        "# Define a function for calculating the moving average\n",
        "def moving_average(data, window_size):\n",
        "  \"\"\"\n",
        "  Calculates the moving average for a time series data.\n",
        "\n",
        "  Args:\n",
        "      data: A list of numerical data points.\n",
        "      window_size: The size of the moving average window.\n",
        "\n",
        "  Returns:\n",
        "      A list of moving average values for each data point.\n",
        "  \"\"\"\n",
        "  moving_averages = []\n",
        "  for i in range(len(data)):\n",
        "    if i < window_size - 1:\n",
        "      moving_averages.append(np.nan)  # Handle initial window with insufficient data\n",
        "    else:\n",
        "      window_data = data[i - window_size + 1 : i + 1]\n",
        "      moving_averages.append(np.mean(window_data))\n",
        "  return moving_averages\n",
        "\n",
        "# Define a function for calculating the standard deviation\n",
        "def standard_deviation(data):\n",
        "  \"\"\"\n",
        "  Calculates the standard deviation of a time series data.\n",
        "\n",
        "  Args:\n",
        "      data: A list of numerical data points.\n",
        "\n",
        "  Returns:\n",
        "      The standard deviation of the data.\n",
        "  \"\"\"\n",
        "  return np.std(data)\n",
        "\n",
        "# Define a function for event detection using Z-score\n",
        "def event_detection_zscore(data, timestamps, threshold=3):\n",
        "  \"\"\"\n",
        "  Detects events in a time series data using Z-score.\n",
        "\n",
        "  Args:\n",
        "      data: A list of numerical data points.\n",
        "      timestamps: A list of timestamps corresponding to data points.\n",
        "      threshold: The Z-score threshold for event detection (default: 3).\n",
        "\n",
        "  Returns:\n",
        "      A list of timestamps where potential events are detected.\n",
        "  \"\"\"\n",
        "  mean = np.mean(data)\n",
        "  std = standard_deviation(data)\n",
        "  zscores = [(x - mean) / std for x in data]\n",
        "  event_timestamps = []\n",
        "  for i, zscore in enumerate(zscores):\n",
        "    if abs(zscore) > threshold:\n",
        "      event_timestamps.append(timestamps[i])\n",
        "  return event_timestamps\n",
        "\n",
        "# Define window size for moving average\n",
        "window_size = 3\n",
        "\n",
        "# Calculate moving average\n",
        "moving_averages = moving_average(data, window_size)\n",
        "\n",
        "# Detect events using Z-score on residuals (difference between data and moving average)\n",
        "residuals = [data[i] - moving_averages[i] for i in range(len(data))]\n",
        "event_timestamps = event_detection_zscore(residuals, timestamps)\n",
        "\n",
        "# Print results\n",
        "print(\"Original Data:\", data)\n",
        "print(\"Moving Average (window size:\", window_size, \"):\", moving_averages)\n",
        "print(\"Residuals:\", residuals)\n",
        "print(\"Event Timestamps (Z-score threshold:\", 3, \"):\", event_timestamps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cq119OO4-4j",
        "outputId": "1854e1f8-7b0d-4a0a-baac-71510b8ec0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data: [10, 12, 15, 18, 20, 17, 15, 12, 8, 5, 3, 2, 1, 4, 8, 12, 15]\n",
            "Moving Average (window size: 3 ): [nan, nan, 12.333333333333334, 15.0, 17.666666666666668, 18.333333333333332, 17.333333333333332, 14.666666666666666, 11.666666666666666, 8.333333333333334, 5.333333333333333, 3.3333333333333335, 2.0, 2.3333333333333335, 4.333333333333333, 8.0, 11.666666666666666]\n",
            "Residuals: [nan, nan, 2.666666666666666, 3.0, 2.333333333333332, -1.3333333333333321, -2.333333333333332, -2.666666666666666, -3.666666666666666, -3.333333333333334, -2.333333333333333, -1.3333333333333335, -1.0, 1.6666666666666665, 3.666666666666667, 4.0, 3.333333333333334]\n",
            "Event Timestamps (Z-score threshold: 3 ): []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Measure Influence via Reachability for a social network using suitable directed graph.**"
      ],
      "metadata": {
        "id": "BCB0jRnBN4qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Example social network data (replace with your data)\n",
        "nodes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
        "edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"A\", \"D\"),\n",
        "         (\"B\", \"C\"), (\"B\", \"E\"), (\"C\", \"D\"),\n",
        "         (\"D\", \"E\"), (\"D\", \"F\"), (\"E\", \"F\")]\n",
        "\n",
        "# Create a directed network graph\n",
        "G = nx.DiGraph()\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Define a function to calculate in-degree (number of incoming edges)\n",
        "def in_degree(G, node):\n",
        "  return len(list(G.predecessors(node)))  # For directed graphs\n",
        "\n",
        "# Define a function to calculate reachability (influenced nodes)\n",
        "def reachability(G, node):\n",
        "  \"\"\"\n",
        "  Calculates the number of nodes reachable from a given node in a directed graph.\n",
        "\n",
        "  Args:\n",
        "      G: NetworkX DiGraph object representing the social network.\n",
        "      node: The node for which to calculate reachability.\n",
        "\n",
        "  Returns:\n",
        "      The number of nodes reachable (influenced) by the given node.\n",
        "  \"\"\"\n",
        "  reachable_nodes = set()\n",
        "  queue = [node]\n",
        "  while queue:\n",
        "    current_node = queue.pop(0)\n",
        "    reachable_nodes.add(current_node)\n",
        "    for neighbor in G.successors(current_node):  # Successors for directed graphs\n",
        "      if neighbor not in reachable_nodes:\n",
        "        queue.append(neighbor)\n",
        "  return len(reachable_nodes) - 1  # Exclude the starting node itself\n",
        "\n",
        "# Example usage\n",
        "influenced_by_a = reachability(G, \"A\")\n",
        "\n",
        "# Print results\n",
        "print(\"Influence of Node A (Reachability):\", influenced_by_a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNSmYImv5Bzx",
        "outputId": "1c247175-c4d1-4acb-95ee-f07b27db6e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Influence of Node A (Reachability): 5\n"
          ]
        }
      ]
    }
  ]
}